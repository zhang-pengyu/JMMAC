<group>
<ul class='breadcrumb'><li><a href='%pathto:matlab;'>Index</a></li><li><a href='%pathto:vl_vlad;'>Prev</a></li><li><a href='%pathto:vl_dsift;'>Next</a></li></ul><div class="documentation"><p>
<a href="%pathto:vl_covdet;">VL_COVDET</a>() implements a number of co-variant feature detectors
(e.g., DoG, Harris-Affine, Harris-Laplace) and corresponding
feature descriptors (SIFT, raw patches).
</p><p>
F = <a href="%pathto:vl_covdet;">VL_COVDET</a>(I) detects upright scale and translation covariant
features based on the Difference of Gaussian (Dog) cornerness
measure from image I (a grayscale image of class SINGLE). Each
column of F is an oriented ellipse (see <a href="%pathto:vl_plotframe;">VL_PLOTFRAME</a>() for the
definition) even if features are upright and/or not affine
covariant (in which case unoriented/circular may suffice).
</p><p>
<a href="%pathto:vl_covdet;">VL_COVDET</a>(I, 'Method', METHOD) allows using one of the following
detection methods instead of the default one:
</p><dl><dt>
DoG
<span class="defaults">default</span></dt><dd><p>
The Difference of Gaussians is an approximate version of the
multiscale trace of Laplacian operator [1].
</p></dd><dt>
Hessian
</dt><dd><p>
Determinant of Hessian operator [2].
</p></dd><dt>
HessianLaplace
</dt><dd><p>
Determinant of Hessian for space localization, trace of
Laplacian for scale detection [2].
</p></dd><dt>
HarrisLaplace
</dt><dd><p>
Harris cornerness measure for space localization, trace
of Laplacian for scale detection [2].
</p></dd><dt>
MultiscaleHessian
</dt><dd><p>
Same as HessianLaplace, but Laplacian scale detection is not
performend (features are simply detected at multiple scales) [2].
</p></dd><dt>
MultiscaleHarris
</dt><dd><p>
Same as HarrisLaplace, but Laplacian scale detection is not
performend (features are simply detected at multiple scales) [2].
</p></dd></dl><p>
The number of detected features is affected by the 'PeakThreshold'
option, which sets the minimum absolute vale of the cornerness
measure to accept a feature. A larger threshold causes fewer
features to be extracted. A good way to choose a threshold is to
look at the cornerness score of the features extracted from an
example image. This score is returned as part of the INFO
structure, as explained below.
</p><p>
In addition to the absolute value of the cornerness measure,
features are also filtered by the curvature of the latter. This is
controlled by the 'EdgeThreshold' parameter, which is the upper
bound on the ratio of the maximum over the minimum curvature of
the cornerness measure at the location of the detected
feature. Intuitively, a low ratio corresponds to an elongated
valley in the cornerness score map, which usually arises from
image edges. These locations are usually discarded as they tend to
be unstable.
</p><p>
Some corner detectors (e.g. HarrisLaplace) use peak in the
response of the multi-scale Laplace operator to select the
scale of the detected frames. These peaks are filtered by
a threshold adjustable by using the 'LaplacianPeakThreshold' option.
</p><p>
<a href="%pathto:vl_covdet;">VL_COVDET</a>(..., 'EstimateAffineShape', true) switches on affine
adaptation, an algorithm [2] that attempts to estimate the affine
covariant shape of each feature.
</p><p>
<a href="%pathto:vl_covdet;">VL_COVDET</a>(..., 'EstimateOrientation', true) switches on the
estimation of the orientation of the features. The algorithm looks
for one or more dominant orientations of the gradient in a patch
around the feature as in [1]. Note that more than one orientation
can be associated to each detected feature, creating multiple
versions of the same feature with different orientations. The maximum
number of orientations per feature can be set with MaxNumOrientations
option.
</p><p>
<a href="%pathto:vl_covdet;">VL_COVDET</a>(..., 'Frames', F) uses the user specified frames F
instead of running a detector. The estimation of the affine shape
and of the feature orientation can still be performed starting
from such frames. Moreover, descriptors for these frames can be
computed.
</p><p>
[F,D] = <a href="%pathto:vl_covdet;">VL_COVDET</a>(I, ...) computes the SIFT descriptors [1] for
the detected features. Each column of D is the descriptor of the
corresponding frame in F. A descriptor is a 128-dimensional vector
of class SINGLE. The same format of <a href="%pathto:vl_sift;">VL_SIFT</a>() is used. SIFT
features are computed on normalized image patches that are
affected by the parameters explained next (for example, in order
to compute SIFT on a larger measurement region, increase the value
of PatchRelativeExtent).
</p><p>
[F,D] = <a href="%pathto:vl_covdet;">VL_COVDET</a>(I, 'descriptor', DESCRIPTOR) allows using one
following descriptors instead
</p><dl><dt>
SIFT
<span class="defaults">default</span></dt><dd><p>
The SIFT descriptor.
</p></dd><dt>
LIOP
</dt><dd><p>
The Local Intensity Order Pattern descriptor. See <a href="%pathto:vl_liop;">VL_LIOP</a>() for
the parameters affecting this descriptor. All LIOP parameters can
be used as input to <a href="%pathto:vl_covdet;">VL_COVDET</a>(), prefixed by the 'Liop' string
(e.g. 'LiopIntensityThrehsold').
</p></dd><dt>
Patch
</dt><dd><p>
Raw patches. In this case, each column of D is a stacked square
image patch. This is very useful to compute alternative
user-defined descriptors.
</p></dd></dl><p>
The following parameters can be used to control the produced
descriptors:
</p><dl><dt>
PatchResolution
<span class="defaults">15 (SIFT) or 20 (LIOP, Patch)</span></dt><dd><p>
The size of the patch R in pixel. Specifically, the patch is a
square image of side 2*R+1 pixels.
</p></dd><dt>
PatchRelativeExtent
<span class="defaults">7.5 (SIFT), 10 (LIOP), or 6 (Patch)</span></dt><dd><p>
The extent E of the patch in the normalized feature frame. The
normalized feature frame is mapped to the feature frame F
detected in the image by a certain affine transformation (A,T)
(see <a href="%pathto:vl_plotframe;">VL_PLOTFRAME</a>() for details). The patch is a square [-E,
E]^2 in the normalize frame, and its shape in the original image
is the (A,T) of it.
</p></dd><dt>
PatchRelativeSmoothing
<span class="defaults">1 (SIFT and LIOP), 1.2 (Patch)</span></dt><dd><p>
The smoothing SIGMA of the patch in the normalized feature
frame. Conceptually, the normalized patch is computed by warping
the image (thought as a continuous signal) by the inverse of the
affine transformation (A,T) discussed above, then by smoothing
the wrapped image by a 2D isotropic Gaussian of standard
deviation SIGMA, and finally by sampling the resulting signal.
</p></dd></dl><p>
[F,D,INFO] = <a href="%pathto:vl_covdet;">VL_COVDET</a>(...) returns an additional structure INFO
with the following members:
</p><dl><dt>
info.peakScores
</dt><dd><p>
The peak scores of the detected features.
</p></dd><dt>
info.edgeScores
</dt><dd><p>
The edge scores of the detected features.
</p></dd><dt>
info.orientationScores
</dt><dd><p>
The peak score of the gradient orientation histograms used to
assign an orientation to the detected features.
</p></dd><dt>
info.laplacianScaleScores
</dt><dd><p>
The peak score of the Laplacian measure used to select
the scale of the detected features.
</p></dd><dt>
info.gss
</dt><dd><p>
The Gaussian scale space (see <a href="%pathto:vl_plotss;">VL_PLOTSS</a>()).
</p></dd><dt>
info.css
</dt><dd><p>
The cornerness measure scale space (see <a href="%pathto:vl_plotss;">VL_PLOTSS</a>()).
</p></dd></dl><p>
In addition to the ones discussed so far, the function supports
the following options:
</p><dl><dt>
NumOctaves
<span class="defaults">maximum possible</span></dt><dd><p>
The number of scale levels sampled per octave when constructing
the scale spaces.
</p></dd><dt>
BaseScale
<span class="defaults">1.6</span></dt><dd><p>
Gaussian Scale Space pyramid base scale. Sets up the blur which is
being applied to the image as sqrt(BS^2 - 0.5^2) where BS is the
value of the base scale and 0.5 is the initial image blur.
</p></dd><dt>
OctaveResolution
<span class="defaults">3</span></dt><dd><p>
The number of scale levels sampled per octave when constructing
the scale spaces.
</p></dd><dt>
DoubleImage
<span class="defaults">true</span></dt><dd><p>
Whether to double the image before extracting features. This
allows to detect features at minimum smoothing level (scale) of
0.5 pixels rather than 1.0, resulting in many more small
features being detected.
</p></dd><dt>
MaxNumOrientations
<span class="defaults">4</span></dt><dd><p>
Maximum number of orientations per feature when EstimateOrientation
is true.
</p></dd><dt>
AllowPaddedWarping
<span class="defaults">true</span></dt><dd><p>
Set to `false` to drop all features where measurement region gets out
of the input image.
</p></dd><dt>
Verbose
</dt><dd><p>
If specified, it increases the verbosity level.
</p></dd><dt>
REFERENCES
</dt></dl><p>
[1] D. G. Lowe, Distinctive image features from scale-invariant
keypoints. IJCV, vol. 2, no. 60, pp. 91-110, 2004.
</p><p>
[2] K. Mikolajcyk and C. Schmid, An affine invariant interest
point detector. ICCV, vol. 2350, pp. 128-142, 2002.
</p><p>
See also: <a href="%pathto:vl_sift;">VL_SIFT</a>(), <a href="%pathto:vl_liop;">VL_LIOP</a>(), <a href="%pathto:vl_plotframe;">VL_PLOTFRAME</a>(), <a href="%pathto:vl_plotss;">VL_PLOTSS</a>(), <a href="%pathto:vl_help;">VL_HELP</a>().
</p></div></group>
